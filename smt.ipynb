{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "from keyfactordiag.feature_selector import FeatureSelector\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smtTrace = pd.read_excel('case/10/6_smt_trace.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedfail = pd.read_excel('case/10/fieldfail_mbsn.xlsx')\n",
    "filedfail['label']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess**\n",
    "1. 刪除LOCATION1為空值的數據\n",
    "2. 刪掉同一個WIP_SN, 同一個LOCATION1會有多筆上料紀錄的location\n",
    "    - 刪除前共xxxx個location\n",
    "    - 刪除後剩下1165個location\n",
    "3. 合併VENDER_CODE+LOT_CODE, 合併VENDER_CODE+DATE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df_smtTrace.dropna(subset=['LOCATION1'], inplace=True)\n",
    "\n",
    "#2\n",
    "location_drop=[]\n",
    "for location in df_smtTrace['LOCATION1'].unique(): \n",
    "    df = df_smtTrace.query(f\"LOCATION1=='{location}'\")\n",
    "    isduplicated = df.duplicated(subset=['WIP_SN','LOCATION1'])\n",
    "    if isduplicated.any():\n",
    "        location_drop.append(location)        \n",
    "df_smtTrace= df_smtTrace[~df_smtTrace['LOCATION1'].isin(location_drop)]    \n",
    "#3\n",
    "df_smtTrace['LOT_CODE']=df_smtTrace['VENDER_CODE']+df_smtTrace['LOT_CODE']\n",
    "df_smtTrace['DATE_CODE']=df_smtTrace['VENDER_CODE']+df_smtTrace['DATE_CODE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**組大表**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigtable={}\n",
    "'''\n",
    "- 目的: 每一個序號的SMT TRACE過站資訊\n",
    "- 作法: \n",
    "    1. filter location1 name\n",
    "    3. label=1, 全都是fail\n",
    "    5. 重新命名欄位名稱        \n",
    "'''\n",
    "\n",
    "col_test = ['SERIAL_NUMBER','IN_STATION_TIME','是否是不良?']\n",
    "col_smttrace = ['WIP_SN','USED_PN','VENDER_CODE','LOT_CODE','DATE_CODE','LOCATION1']\n",
    "col_output = ['WIP_SN','label','USED_PN','VENDER_CODE','LOT_CODE','DATE_CODE']\n",
    "for location in tqdm(df_smtTrace['LOCATION1'].unique()): #['0UC2,0UC2,', '0PUG101,0PUG101,']:\n",
    "#for location in ['0PUZ4,0PUZ2,0PUZ3,0PUZ4,0PUZ2,0PUZ3,', '0PUG101,0PUG101,']:\n",
    "    #step 1 \n",
    "    df_smttrace = df_smtTrace.query(f\"LOCATION1=='{location}'\")[col_smttrace]\n",
    "    #step 2 \n",
    "    #step 3\n",
    "    df_smttrace['label']=1 #Fail\n",
    "    #step 4 \n",
    "    #step 5\n",
    "    df_smttrace = df_smttrace[col_output]\n",
    "    _col_rename = dict(zip(col_smttrace, [f'smttrace:{location}:{f}' if f not in ['WIP_SN','label'] else f for f in col_smttrace]))\n",
    "    df_smttrace.rename(columns=_col_rename, inplace=True)\n",
    "    bigtable[f'smt_trace:{location}'] = df_smttrace\n",
    "    \n",
    "df_bigtable = filedfail \n",
    "for k, data in tqdm(bigtable.items()):\n",
    "    df_bigtable = pd.merge(df_bigtable, data, on=['WIP_SN','label'], how='left')\n",
    "bigtable['big'] = df_bigtable    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**集中性分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====== Factor Ranking =======\n",
    "def getentropy(df, feature, isonehot=True):\n",
    "    if isonehot:\n",
    "        data_f = df[df[feature]==1]\n",
    "        len_f1=len(data_f)\n",
    "        p_data1 = data_f['label'].value_counts() \n",
    "        ent1 = sc.stats.entropy(p_data1/len_f1, base=2)\n",
    "        data_f = df[df[feature]==0]\n",
    "        len_f0=len(data_f)\n",
    "        p_data0 = data_f['label'].value_counts()\n",
    "        ent0 = sc.stats.entropy(p_data0/len_f0, base=2)\n",
    "\n",
    "        ent_mean = ent1*(len_f1/(len_f1+len_f0))+ent0*(len_f0/(len_f1+len_f0))\n",
    "        p_data1.rename(index={1:'Fail',0:'Pass'}, inplace=True)\n",
    "        p_data0.rename(index={1:'Fail',0:'Pass'}, inplace=True)\n",
    "        return (ent1, p_data1, ent0, p_data0, ent_mean)\n",
    "    else:\n",
    "        entlist=[]\n",
    "        for v in df[feature].unique():\n",
    "            data_f = df[df[feature]==v]\n",
    "            len_f=len(data_f)\n",
    "            p_data = data_f['label'].value_counts() \n",
    "            ent_ = sc.stats.entropy(p_data/len_f, base=2)\n",
    "            entlist.append(ent_*len_f/len(df))\n",
    "        ent = np.sum(entlist)\n",
    "        return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_bigtable_ = bigtable['big']\n",
    "##FIXME 暫時移除時間和SN的欄位\n",
    "newcols= list(filter(lambda x: 'TIME' not in x, df_bigtable_.columns))\n",
    "newcols= list(filter(lambda x: 'KEY_PART_SN' not in x, newcols))\n",
    "df_bigtable = df_bigtable_[newcols]\n",
    "train_labels = df_bigtable['label'].copy()\n",
    "train = df_bigtable.drop(columns = ['label'])\n",
    "del train['WIP_SN']\n",
    "fs = FeatureSelector(data = train, labels = train_labels)\n",
    "\n",
    "#remove不能用的因子 (missing value>30%, single unique)\n",
    "fs.identify_missing(missing_threshold=0.3)\n",
    "fs.identify_single_unique()\n",
    "train = fs.remove(methods = ['missing', 'single_unique'])\n",
    "print(fs.check_removal())\n",
    "fs = FeatureSelector(data = train, labels = train_labels)        \n",
    "\n",
    "#One-Hot Encoding\n",
    "fs.identify_zero_importance(task = 'classification', eval_metric = 'auc', n_iterations = 1, early_stopping = True)\n",
    "one_hot_features = fs.one_hot_features\n",
    "base_features = fs.base_features\n",
    "print('There are %d original features' % len(base_features))\n",
    "print('There are %d one-hot features' % len(one_hot_features))        \n",
    "\n",
    "#Layer 1 factor analysis \n",
    "ocols=['feature','Qty{P}','Qty{N}','entropy(mean)']\n",
    "df_entdata = pd.concat([fs.data_all[one_hot_features], train_labels], axis=1)\n",
    "rootent = sc.stats.entropy(df_entdata['label'].value_counts()/len(df_entdata['label']), base=2)\n",
    "print('root ent:',rootent)\n",
    "entlist = []\n",
    "for f in tqdm(df_entdata.columns):\n",
    "    if f=='label':\n",
    "        continue\n",
    "    ent1, p_data1, ent0, p_data0, ent = getentropy(df_entdata, f)\n",
    "    #entlist.append([f, ent1, str(dict(p_data1.sort_index(ascending=False))), ent0, str(dict(p_data0.sort_index(ascending=False))), ent])\n",
    "    entlist.append([f, ent1, dict(p_data1.sort_index(ascending=False)), ent0, dict(p_data0.sort_index(ascending=False)), ent])\n",
    "\n",
    "df_ent = pd.DataFrame(entlist)\n",
    "df_ent.columns = ['feature','entropy(P)','Qty(P)','entropy(N)','Qty(N)','entropy(mean)']\n",
    "#df_ent['exp'] = df_ent['entropy(mean)'].map(lambda x: (rootent-x)/rootent) #exp: 解釋了多少比例的不確定性\n",
    "#df_ent = df_ent.sort_values(by='entropy(mean)').reset_index(drop=True)\n",
    "#self.factortable['main']=df_ent\n",
    "\n",
    "\n",
    "#--- output---\n",
    "def getothers(x, cols):\n",
    "    k=':'.join(x.split(':')[:3])\n",
    "    ret = list(filter(lambda x: k in x, cols))\n",
    "    ret = [c.split(':')[-1] for c in ret]\n",
    "    return ret        \n",
    "\n",
    "outputcols=['Type', 'Factor A', 'Factor B', 'Factor C', 'Pass', 'Fail','Failrate', 'Others','Others-Pass', 'Others-Fail', 'entropy(mean)','rank']\n",
    "\n",
    "#Table格式 ##FIXME\n",
    "df = df_ent.copy()\n",
    "cols = df['feature']\n",
    "df['Type']=df['feature'].map(lambda x: x.split(':')[0])\n",
    "df['Factor A']=df['feature'].map(lambda x: x.split(':')[1])\n",
    "df['Factor B']=df['feature'].map(lambda x: x.split(':')[2])\n",
    "df['Factor C']=df['feature'].map(lambda x: x.split(':')[3])\n",
    "df['Others'] = df['feature'].map(lambda x: getothers(x, cols))\n",
    "del df['feature']\n",
    "df['Pass']=df['Qty(P)'].map(lambda x: x.get('Pass',0))\n",
    "df['Fail']=df['Qty(P)'].map(lambda x: x.get('Fail',0) )\n",
    "df['Failrate']=df['Fail']/(df['Pass']+df['Fail'])\n",
    "df['Failrate'] = df['Failrate'].map(lambda x: np.round(x,2))\n",
    "df['Others-Pass']=df['Qty(N)'].map(lambda x: x.get('Pass',0))\n",
    "df['Others-Fail']=df['Qty(N)'].map(lambda x: x.get('Fail',0) )\n",
    "del df['Qty(P)']\n",
    "del df['Qty(N)']\n",
    "del df['entropy(P)']\n",
    "del df['entropy(N)']\n",
    "df.sort_values(by='Fail', ascending=False, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index':'rank'}, inplace=True)            \n",
    "df = df[outputcols]\n",
    "\n",
    "\n",
    "writer=pd.ExcelWriter('output/case10_集中性分析Report.xlsx') \n",
    "df.to_excel(writer, 'main', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
